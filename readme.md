# Desafio de c√≥digo BGC Brasil

Este reposit√≥rio cont√©m o desafio t√©cnico para a **BGC Brasil**. O objetivo √© realizar o web scraping dos produtos mais vendidos da Amazon, persistir os dados em um banco NoSQL (DynamoDB) e disponibilizar uma API Serverless para consulta.

---

## üõ† Tecnologias Utilizadas

### Scraper
- **Node.js v24:** Aproveitando o suporte nativo a TypeScript (sem necessidade de compiladores externos).
- **Puppeteer + Stealth Plugin:** Para automa√ß√£o de browser com t√©cnicas de evas√£o de anti-bot.
- **Zod:** Valida√ß√£o rigorosa de vari√°veis de ambiente e esquemas de dados.

### API (Serverless)
- **Node.js v20:** Runtime otimizado para AWS Lambda.
- **Serverless Framework v3:** Orquestra√ß√£o de infraestrutura como c√≥digo (IaC).
- **AWS Lambda & API Gateway:** Computa√ß√£o escal√°vel e gest√£o de endpoints.
- **AWS DynamoDB:** Banco de dados NoSQL de baixa lat√™ncia.
- **Esbuild:** Bundler extremamente r√°pido para minimizar o tempo de cold start.

---

## üèó Decis√µes de Arquitetura e Design

### 1. Organiza√ß√£o do Reposit√≥rio
Optei por uma estrutura simples de pastas (`/api` e `/scraper`) em vez de ferramentas como *Monorepos (Turbo/Workspaces)*. Dado o escopo enxuto do projeto, essa abordagem evita complexidade desnecess√°ria e repeti√ß√£o de configura√ß√µes pesadas, mantendo o projeto √°gil.

### 2. Inje√ß√£o de Depend√™ncia Manual
Implementei o padr√£o **Repository** e **Inje√ß√£o de Depend√™ncia** em ambos os projetos:
- **API:** A inje√ß√£o √© feita manualmente no handler. Evitei frameworks de DI (como Inversify ou NestJS) para garantir que o **Cold Start** da Lambda seja o menor poss√≠vel, reduzindo lat√™ncia e custos operacionais.
- **Scraper:** A l√≥gica de extra√ß√£o √© separada dos providers de dados. Isso permite trocar a Amazon pelo Mercado Livre, por exemplo, alterando apenas o provider, sem tocar na l√≥gica de neg√≥cio.

### 3. Estrat√©gia de Scraping & Anti-Bot
A Amazon possui mecanismos rigorosos contra automa√ß√£o. Para mitigar bloqueios:
- Utilizei o `puppeteer-extra-plugin-stealth` para mascarar as propriedades do navegador.
- Implementei **delay aleat√≥rio**, simula√ß√£o de **movimenta√ß√£o de mouse** e **scroll suave** na p√°gina.
- O scraping √© executado **sequencialmente**. Embora o paralelismo fosse mais r√°pido, a execu√ß√£o sequencial reduz o risco de *throttling* (bloqueio por excesso de requisi√ß√µes) do IP.
- O scraper percorre as categorias principais e extrai o **Top 3** de cada uma.

### 4. Padroniza√ß√£o
- **Biome:** Utilizado para Linting e Formata√ß√£o, garantindo um c√≥digo limpo e perform√°tico.
- **Conventional Commits:** Hist√≥rico de versionamento organizado em ingl√™s para facilitar o rastreamento de mudan√ßas.

---

## üöÄ Como Executar Localmente

### Pr√©-requisitos
- Node.js v20+
- Docker (opcional, para o DynamoDB Local) ou Plugin Serverless DynamoDB Local instalado.

### 1. Configura√ß√£o da API
```bash
cd api
npm install
# Iniciar o DynamoDB Local e a API
npx sls dynamodb install
npx sls offline start --stage local
```
A API estar√° dispon√≠vel em `http://localhost:3000`.

### 2. Configura√ß√£o do Scraper
```bash
cd scraper
npm install
# Configure o seu .env com base no .env.example
npm run start
```

---

## ‚òÅÔ∏è Deploy e Popula√ß√£o na Nuvem

### Deploy da Infraestrutura
Dentro da pasta `/api`:
```bash
npx sls deploy --stage dev
```

### Popula√ß√£o do Banco Remoto
Ap√≥s o deploy, voc√™ pode usar o scraper local para popular o banco de dados na AWS:
1. No arquivo `.env` do scraper, altere para os dados de produ√ß√£o.
2. Certifique-se de que suas credenciais AWS est√£o configuradas no ambiente.
3. Execute o scraper: `npm run start`.

---

## üì° Documenta√ß√£o da API

### Listar Produtos
`GET /products`

**Filtro por Categoria:**
`GET /products?category=Eletr√¥nicos`

**Exemplo de Resposta:**
```json
{
  "data": [
    {
      "id": "uuid",
      "name": "Echo Dot (5¬™ Gera√ß√£o)",
      "price": 439.00,
      "category": "Eletr√¥nicos",
      "rank": 1,
      "url": "https://amazon.com.br/...",
      "createdAt": "2025-12-19T..."
    }
  ]
}
```

**Testando com cURL:**
```bash
curl "(url em produ√ß√£o ou localhost)/products?category=(categoria desejada)"
```
